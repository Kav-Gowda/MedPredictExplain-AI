# -*- coding: utf-8 -*-
"""MedPredictExplain-AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jMgxGsugvYcVgyfZH99XGwpyuUK9KL8_
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install tqdm==4.66.4
# %pip install pandas==2.1.4
# %pip install matplotlib==3.8.0
# %pip install seaborn==0.13.2
# %pip install scikit-learn==1.5.0
# %pip install ucimlrepo==0.0.7
# %pip install missingno==0.4.2
#

from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# Suppress warnings for cleaner output
def warn(*args, **kwargs): pass
warnings.warn = warn
warnings.filterwarnings('ignore')

sns.set_context('notebook')
sns.set_style('white')

def plot_histograms(X, y=None):
    """
    Creates distribution plots for numerical and categorical features,
    optionally grouped by the target variable.
    """
    if y is not None:
        X = X.copy()
        X['label'] = y

    for col in X.columns:
        plt.figure(figsize=(8, 6))
        if X[col].dtype in ['float64', 'int64']:
            sns.histplot(data=X, x=col, hue='label', kde=False, bins=30, alpha=0.5, edgecolor="black")
        else:
            sns.countplot(data=X, x=col, hue='label', dodge=False, alpha=0.8, edgecolor="black")
        plt.title(f"Distribution of {col}")
        plt.tight_layout()
        plt.show()

from ucimlrepo import fetch_ucirepo

# Fetch dataset
heart_disease = fetch_ucirepo(id=45)
X = heart_disease.data.features
y = heart_disease.data.targets

print("Dataset loaded successfully.")
print(X.head())
print("\nShape:", X.shape)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X['ca'] = imputer.fit_transform(X[['ca']])
X['thal'] = imputer.fit_transform(X[['thal']])
print("Missing values handled successfully.")

corr_matrix = X.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", vmin=-1, vmax=1)
plt.title("Feature Correlation Matrix")
plt.show()

y = y.map(lambda x: 1 if x in {1, 2, 3} else 0)

plt.figure(figsize=(8, 6))
y.value_counts().plot(kind='bar', alpha=0.7, color='skyblue')
plt.title("Label Distribution")
plt.xlabel("Label")
plt.ylabel("Count")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

data = X.copy()
data['target'] = y
mean_table = data.groupby('target').mean()
del data
mean_table

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

y_pred = logistic_model.predict(X_test)
print("Logistic Regression Report:")
print(classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_estimator(logistic_model, X_test, y_test, cmap="Blues")
plt.title("Confusion Matrix: Logistic Regression")
plt.show()

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(random_state=42, n_estimators=100)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print("Random Forest Report:")
print(classification_report(y_test, y_pred_rf))

ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, cmap="Oranges")
plt.title("Confusion Matrix: Random Forest")
plt.show()

plt.figure(figsize=(8, 6))
importances = pd.Series(rf_model.feature_importances_, index=X.columns)
importances.sort_values().plot(kind='bar', color='orange')
plt.title("Random Forest Feature Importances")
plt.xlabel("Features")
plt.ylabel("Importance Score")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

rf_model = RandomForestClassifier(random_state=42, n_estimators=100)
rf_model.fit(X, y)

feature_of_interest = "age"
grid_points = 50
Xs = np.linspace(X[feature_of_interest].min(), X[feature_of_interest].max(), grid_points)

partial_dependence = np.zeros(grid_points)
temp_X = X.copy()

for n, xs in enumerate(Xs):
    temp_X[feature_of_interest] = xs
    predictions = rf_model.predict_proba(temp_X)[:, 1]
    partial_dependence[n] = predictions.mean()

plt.figure(figsize=(8, 6))
plt.plot(Xs, partial_dependence, label=f"PDP for {feature_of_interest}", color="blue")
plt.xlabel(f"{feature_of_interest} Value")
plt.ylabel("Average Prediction")
plt.title("Partial Dependence Plot (Manual)")
plt.legend()
plt.grid()
plt.show()

from sklearn.inspection import PartialDependenceDisplay

fig, ax = plt.subplots(figsize=(10, 6))
PartialDependenceDisplay.from_estimator(
    rf_model, X_train, features=[0], feature_names=X.columns, grid_resolution=50, ax=ax
)
plt.suptitle("Partial Dependence Plot (Random Forest)", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

feature_names = X.columns

for i, feature_name in enumerate(feature_names):
    print(f"Processing Feature: {feature_name}")
    if feature_name not in ['sex', 'fbs', 'exang']:
        fig, ax = plt.subplots(figsize=(10, 6))
        PartialDependenceDisplay.from_estimator(
            rf_model, X, features=[i], feature_names=feature_names, grid_resolution=50, ax=ax
        )
        plt.suptitle(f"PDP for Feature: {feature_name}", fontsize=16)
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()

features_to_plot = ['thalach', 'chol', ('thalach', 'chol')]
fig, ax = plt.subplots(figsize=(12, 6))
PartialDependenceDisplay.from_estimator(
    rf_model, X_train, features=features_to_plot, grid_resolution=50, ax=ax
)
plt.suptitle("2D Partial Dependence Plot: Thalach vs Cholesterol", fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

clf = LogisticRegression(random_state=0).fit(X, y)
PartialDependenceDisplay.from_estimator(clf, X, features=[0, 1, (0, 1)])
plt.suptitle("Partial Dependence Plot (Logistic Regression)", fontsize=16)
plt.show()



